# Vision- und Audio-AI-Apps

In dieser Lektion erfÃ¤hrst du, wie Vision-AI deinen Apps ermÃ¶glicht, Bilder zu generieren und zu interpretieren. Audio-AI bietet deinen Apps die MÃ¶glichkeit, Audio zu erzeugen und es sogar in Echtzeit zu transkribieren.

---

## Vision

[![Vision AI-ErklÃ¤rung](https://img.youtube.com/vi/QXbASt1KXuw/0.jpg)](https://youtu.be/QXbASt1KXuw?feature=shared)

_â¬†ï¸Klicke auf das Bild, um das Video anzusehenâ¬†ï¸_

Vision-basierte AI-AnsÃ¤tze werden verwendet, um Bilder zu generieren und zu interpretieren. Dies kann fÃ¼r eine Vielzahl von Anwendungen nÃ¼tzlich sein, wie z. B. Bilderkennung, Bildgenerierung und Bildbearbeitung. Aktuelle Modelle sind multimodal, was bedeutet, dass sie verschiedene Eingaben wie Text, Bilder und Audio akzeptieren und unterschiedliche Ausgaben erzeugen kÃ¶nnen. In diesem Fall konzentrieren wir uns auf die Bilderkennung.

### Bilderkennung mit MEAI

Bilderkennung bedeutet mehr, als dass das AI-Modell dir sagt, was es in einem Bild erkennt. Du kannst dem Modell auch Fragen zum Bild stellen, zum Beispiel: _Wie viele Personen sind zu sehen, und regnet es?_

Okay â€“ wir werden das Modell auf die Probe stellen und es fragen, wie viele rote Schuhe auf dem ersten Foto zu sehen sind, und es dann eine Quittung auf Deutsch analysieren lassen, damit wir wissen, wie viel Trinkgeld wir geben sollen.

![Eine Komposition mit beiden Bildern, die im Beispiel verwendet werden. Das erste zeigt mehrere LÃ¤ufer, aber nur deren Beine. Das zweite ist eine deutsche Restaurantquittung](../../../translated_images/example-visual-image.e2fc4ffa5f01b3d65bb9bd5d23eebf97513bf486b761209b28fea06b63a11f6c.de.png)

> ğŸ§‘â€ğŸ’»**Beispielcode**: Du kannst [dem Beispielcode hier folgen](../../../03-CoreGenerativeAITechniques/src/Vision-01MEAI-GitHubModels).

1. Wir verwenden MEAI und GitHub-Modelle, also instanziere die `IChatClient` wie gewohnt. Beginne auch damit, eine Chat-Historie zu erstellen.

    ```csharp
    IChatClient chatClient = new ChatCompletionsClient(
        endpoint: new Uri("https://models.inference.ai.azure.com"),
        new AzureKeyCredential(githubToken)) // make sure to grab githubToken from the secrets or environment
    .AsChatClient("gpt-4o-mini");

    List<ChatMessage> messages = 
    [
        new ChatMessage(ChatRole.System, "You are a useful assistant that describes images using a direct style."),
        new ChatMessage(ChatRole.User, "How many red shoes are in the photo?") // we'll start with the running photo
    ];
    ```

1. Im nÃ¤chsten Schritt wird das Bild in ein `AIContent`-Objekt geladen, das als Teil unseres GesprÃ¤chs verwendet wird. Dieses wird dann an das Modell gesendet, damit es fÃ¼r uns beschrieben wird.

    ```csharp
    var imagePath = "FULL PATH TO THE IMAGE ON DISK";

    AIContent imageContent = new DataContent(File.ReadAllBytes(imagePath), "image/jpeg"); // the important part here is that we're loading it in bytes. The image could come from anywhere.

    var imageMessage = new ChatMessage(ChatRole.User, [imageContent]);

    messages.Add(imageMessage);

    var response = await chatClient.GetResponseAsync(messages);

    messages.Add(response.Message);

    Console.WriteLine(response.Message.Text);
    ```

1. Danach lassen wir das Modell die Restaurantquittung analysieren â€“ die auf Deutsch ist â€“, um herauszufinden, wie viel Trinkgeld wir geben sollten:

    ```csharp
    // this will go after the previous code block
    messages.Add(new ChatMessage(ChatRole.User, "This is a receipt from a lunch. I had the sausage. How much of a tip should I leave?"));

    var receiptPath = "FULL PATH TO THE RECEIPT IMAGE ON DISK";

    AIContent receiptContent = new DataContent(File.ReadAllBytes(receiptPath), "image/jpeg");
    var receiptMessage = new ChatMessage(ChatRole.User, [receiptContent]);

    messages.Add(receiptMessage);

    response = await chatClient.GetResponseAsync(messages);
    messages.Add(response.Message);

    Console.WriteLine(response.Message.Text);
    ```

Hier mÃ¶chte ich einen wichtigen Punkt hervorheben. Wir fÃ¼hren ein GesprÃ¤ch mit einem Sprachmodell, genauer gesagt mit einem multimodalen Modell, das sowohl Text- als auch Bild- (und Audio-)Interaktionen verarbeiten kann. Und wir setzen das GesprÃ¤ch mit dem Modell wie gewohnt fort. Sicher, es ist ein anderer Objekttyp, den wir an das Modell senden, `AIContent` instead of a `string`, but the workflow is the same.

> ğŸ™‹ **Need help?**: If you encounter any issues, [open an issue in the repository](https://github.com/microsoft/Generative-AI-for-beginners-dotnet/issues/new).

## Audio AI

[![Audio AI explainer video](https://img.youtube.com/vi/fuquPXRNqCo/0.jpg)](https://youtu.be/fuquPXRNqCo?feature=shared)

_â¬†ï¸Click the image to watch the videoâ¬†ï¸_

Real-time audio techniques allow your apps to generate audio and transcribe it in real-time. This can be useful for a wide range of applications, such as voice recognition, speech synthesis, and audio manipulation.

But we're going to have to transition away from MEAI and from the model we were using to Azure AI Speech Services.

To setup an Azure AI Speech Service model, [follow these directions](../02-SetupDevEnvironment/getting-started-azure-openai.md) but instead of choosing an OpenAI model, choose **Azure-AI-Speech**.

> **ğŸ—’ï¸Note:>** Audio is coming to MEAI, but as of this writing isn't available yet. When it is available we'll update this course.

### Implementing speech-to-text with Cognitive Services

You'll need the **Microsoft.CognitiveServices.Speech** NuGet package for this example.

> ğŸ§‘â€ğŸ’»**Sample code**: You can follow [along with sample code here](../../../03-CoreGenerativeAITechniques/src/Audio-01-SpeechMic).

1. The first thing we'll do (after grabbing the key and region of the model's deployment) is instantiate a `SpeechTranslationConfig`-Objekt. Dies ermÃ¶glicht es uns, dem Modell mitzuteilen, dass wir gesprochenes Englisch aufnehmen und in geschriebenes Spanisch Ã¼bersetzen.

    ```csharp
    var speechKey = "<FROM YOUR MODEL DEPLOYMENT>";
    var speechRegion = "<FROM YOUR MODEL DEPLOYMENT>";

    var speechTranslationConfig = SpeechTranslationConfig.FromSubscription(speechKey, speechRegion);
    speechTranslationConfig.SpeechRecognitionLanguage = "en-US";
    speechTranslationConfig.AddTargetLanguage("es-ES");
    ```

1. Als NÃ¤chstes benÃ¶tigen wir Zugriff auf das Mikrofon und erstellen ein neues `TranslationRecognizer`-Objekt, das die Kommunikation mit dem Modell Ã¼bernimmt.

    ```csharp
    using var audioConfig = AudioConfig.FromDefaultMicrophoneInput();
    using var translationRecognizer = new TranslationRecognizer(speechTranslationConfig, audioConfig);
    ```

1. SchlieÃŸlich rufen wir das Modell auf und richten eine Funktion ein, um dessen RÃ¼ckgabe zu verarbeiten.
   
    ```csharp
    var translationRecognitionResult = await translationRecognizer.RecognizeOnceAsync();
    OutputSpeechRecognitionResult(translationRecognitionResult);

    void OutputSpeechRecognitionResult(TranslationRecognitionResult translationRecognitionResult)
    {
        switch (translationRecognitionResult.Reason)
        {
            case ResultReason.TranslatedSpeech:
                Console.WriteLine($"RECOGNIZED: Text={translationRecognitionResult.Text}");
                foreach (var element in translationRecognitionResult.Translations)
                {
                    Console.WriteLine($"TRANSLATED into '{element.Key}': {element.Value}");
                }
                break;
            case ResultReason.NoMatch:
                // handle when speech could not be recognized
                break;
            case ResultReason.Canceled:
                // handle an error condition
                break;
        }
    }
    ```

Die Verwendung von AI zur Verarbeitung von Audio unterscheidet sich etwas von dem, was wir bisher getan haben, da wir Azure AI Speech Services nutzen. Die Ergebnisse der Ãœbersetzung von gesprochenem Audio in Text sind jedoch beeindruckend.

> ğŸ™‹ **Brauchst du Hilfe?**: Falls du auf Probleme stÃ¶ÃŸt, [erÃ¶ffne ein Issue im Repository](https://github.com/microsoft/Generative-AI-for-beginners-dotnet/issues/new).

Wir haben ein weiteres Beispiel, das [zeigt, wie man Echtzeit-AudiogesprÃ¤che mit Azure Open AI fÃ¼hrt](../../../03-CoreGenerativeAITechniques/src/Audio-02-RealTimeAudio) â€“ schau es dir an!


## ZusÃ¤tzliche Ressourcen

- [Bilder mit AI und .NET generieren](https://learn.microsoft.com/dotnet/ai/quickstarts/quickstart-openai-generate-images?tabs=azd&pivots=openai)


## Als NÃ¤chstes

Du hast gelernt, wie du Vision- und Audio-Funktionen zu deinen .NET-Anwendungen hinzufÃ¼gen kannst. In der nÃ¤chsten Lektion erfÃ¤hrst du, wie du AI erstellst, die eine gewisse Autonomie besitzt.

ğŸ‘‰ [Schau dir AI-Agents an](./04-agents.md).

**Haftungsausschluss**:  
Dieses Dokument wurde mithilfe von KI-gestÃ¼tzten maschinellen Ãœbersetzungsdiensten Ã¼bersetzt. Obwohl wir uns um Genauigkeit bemÃ¼hen, weisen wir darauf hin, dass automatisierte Ãœbersetzungen Fehler oder Ungenauigkeiten enthalten kÃ¶nnen. Das Originaldokument in seiner ursprÃ¼nglichen Sprache sollte als maÃŸgebliche Quelle betrachtet werden. FÃ¼r kritische Informationen wird eine professionelle menschliche Ãœbersetzung empfohlen. Wir Ã¼bernehmen keine Haftung fÃ¼r MissverstÃ¤ndnisse oder Fehlinterpretationen, die durch die Nutzung dieser Ãœbersetzung entstehen.